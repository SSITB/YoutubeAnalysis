{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Comments Sentiment Analysis \n",
    "Spring 2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Basic Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import pandas as pd; import os\n",
    "import csv; import numpy as np\n",
    "import re; import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/andiedonovan/myProjects/Youtube_Python_Project/AndiesBranch/') # change directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data.csv', delimiter=\",\", skiprows=2, encoding='utf-8', engine='python') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Labeled YouTube Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "okgo = pd.read_csv('data/OKGO.csv', delimiter=\";\", skiprows=2, encoding='latin-1', engine='python') # read in the data\n",
    "trump = pd.read_csv('data/trump.csv', delimiter=\",\", skiprows=2, encoding='utf-8', error_bad_lines=False, engine='python') \n",
    "swift = pd.read_csv('data/TaylorSwift.csv', delimiter=\",\", skiprows=2, nrows=180, encoding='utf-8', engine='python') \n",
    "royal = pd.read_csv('data/RoyalWedding.csv', delimiter=\",\", skiprows=2, nrows=61, encoding='utf-8', engine='python')\n",
    "paul = pd.read_csv('data/LoganPaul.csv', delimiter=\",\", skiprows=2, nrows=200, encoding='utf-8', engine='python') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-YouTube Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs = pd.read_csv('data/Kagel.csv', delimiter=\",\", skiprows=2, encoding='latin-1', engine='python') # read in the data\n",
    "tweets = pd.read_csv('data/twitter.csv', delimiter=\",\", skiprows=2, encoding='latin-1', engine='python') # read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Clean Data Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['Topic', 'TweetId', \"TweetDate\"], axis = 1).dropna()\n",
    "tweets.columns = [\"label\", \"comment\"]\n",
    "tweets.label = tweets.label.replace({'positive': '1.0', 'negative':'-1.0', 'neutral': '0.0', 'irrelevant': '0.0'}, regex=True)\n",
    "tweets['label'] = pd.to_numeric(tweets['label'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs.columns = [\"label\", \"comment\"]\n",
    "blogs['label'] = pd.to_numeric(blogs['label'], errors='coerce')\n",
    "okgo = okgo.drop(okgo.columns[[2, 3]], axis =1).dropna()\n",
    "paul = paul.drop([\"Unnamed: 2\"], axis =1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_cols(DF):\n",
    "    DF.columns = [\"label\", \"comment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_cols(okgo)\n",
    "fix_cols(trump)\n",
    "fix_cols(swift)\n",
    "fix_cols(royal)\n",
    "fix_cols(paul)\n",
    "fix_cols(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.b Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = pd.concat([okgo, trump, swift, royal, paul], ignore_index=True)\n",
    "full = pd.concat([okgo, trump, swift, royal, paul, blogs, tweets], ignore_index=False)\n",
    "videos_not_okgo = pd.concat([trump, swift, royal, paul], ignore_index=False)\n",
    "videos_not_royal = pd.concat([okgo, trump, swift, paul], ignore_index=False)\n",
    "\n",
    "DataList = [videos, full, videos_not_royal, videos_not_okgo]\n",
    "excluded = [okgo, royal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>0.0</td>\n",
       "      <td>waiting for some music,still waiting....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Mayo 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>1.0</td>\n",
       "      <td>As always awesome song _ and visually pleasing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Lol \"Phuket\" Thailand XD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Try not to get satisfied video right here. I m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            comment\n",
       "857     0.0           waiting for some music,still waiting....\n",
       "2236    0.0                                          Mayo 2018\n",
       "929     1.0  As always awesome song _ and visually pleasing...\n",
       "67      1.0                           Lol \"Phuket\" Thailand XD\n",
       "430     0.0  Try not to get satisfied video right here. I m..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = videos.copy()\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Remove Non-Alphabetic Characters (including numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AsStr(DF):\n",
    "    DF[\"comment\"]= DF[\"comment\"].astype(str) \n",
    "\n",
    "for i in range(0, len(DataList)): \n",
    "    AsStr(DataList[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(excluded)): \n",
    "    AsStr(excluded[i])\n",
    "    \n",
    "AsStr(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanerFn(b):\n",
    "    for row in range(len(b)):\n",
    "        line = b.loc[row, \"comment\"]\n",
    "        b.loc[row,\"comment\"] = re.sub(\"[^a-zA-Z]\", \" \", line)\n",
    "        \n",
    "def cleanerFn2(b):\n",
    "    for row in range(len(b)):\n",
    "        line = b.iloc[row, 1]\n",
    "        b.iloc[row,1] = re.sub(\"[^a-zA-Z]\", \" \", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanerFn(df)\n",
    "cleanerFn2(data)\n",
    "\n",
    "for i in range(0, len(DataList)): \n",
    "    cleanerFn2(DataList[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(excluded)): \n",
    "    cleanerFn2(excluded[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andiedonovan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "nltk.download('stopwords')\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['com_token']=df['comment'].str.lower().str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Remove Stop Words, Lemmatization, Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/andiedonovan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlpFunction(DF):\n",
    "    DF['com_token'] = DF['comment'].str.lower().str.split()\n",
    "    DF['com_remv'] = DF['com_token'].apply(lambda x: [y for y in x if y not in sw])\n",
    "    DF[\"com_lemma\"] = DF['com_remv'].apply(lambda x : [lemmatizer.lemmatize(y) for y in x]) # lemmatization\n",
    "    DF['com_stem'] = DF['com_lemma'].apply(lambda x : [ps.stem(y) for y in x]) # stemming\n",
    "    DF[\"com_full\"] = DF[\"com_stem\"].apply(' '.join)\n",
    "    DF[\"com_tagged\"] = DF['comment'].apply(lambda x : [nltk.pos_tag(y) for y in x]) #word tagging\n",
    "    DF[\"com_stem_str\"] = DF[\"com_stem\"].apply(', '.join)\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to count bigrams and POS tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"com_tagged\"] = df['comment'].apply(lambda x : [nltk.pos_tag(y) for y in x]) #word tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andiedonovan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "nltk.download('punkt')\n",
    "\n",
    "texts = df['comment'].tolist()\n",
    "tagged_texts = pos_tag_sents(map(word_tokenize, texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged_texts\n",
    "df[\"tagged\"] = tagged_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>com_token</th>\n",
       "      <th>com_tagged</th>\n",
       "      <th>tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>omgidk y but i just cant stop laughinglmao</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[(n, NN)], [(a, DT)], [(n, NN)]]</td>\n",
       "      <td>[(nan, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There is literally not one thing about Krystal...</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[(n, NN)], [(a, DT)], [(n, NN)]]</td>\n",
       "      <td>[(nan, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[(n, NN)], [(a, DT)], [(n, NN)]]</td>\n",
       "      <td>[(nan, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Try not to laugh challenge right there</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[(n, NN)], [(a, DT)], [(n, NN)]]</td>\n",
       "      <td>[(nan, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>httpswwwyoutubecomwatchvKtI40e1cEN4</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[(n, NN)], [(a, DT)], [(n, NN)]]</td>\n",
       "      <td>[(nan, NN)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label comment com_token  \\\n",
       "0        omgidk y but i just cant stop laughinglmao      nan     [nan]   \n",
       "1  There is literally not one thing about Krystal...     nan     [nan]   \n",
       "2                                                        nan     [nan]   \n",
       "3            Try not to laugh challenge right there      nan     [nan]   \n",
       "4               httpswwwyoutubecomwatchvKtI40e1cEN4      nan     [nan]   \n",
       "\n",
       "                          com_tagged       tagged  \n",
       "0  [[(n, NN)], [(a, DT)], [(n, NN)]]  [(nan, NN)]  \n",
       "1  [[(n, NN)], [(a, DT)], [(n, NN)]]  [(nan, NN)]  \n",
       "2  [[(n, NN)], [(a, DT)], [(n, NN)]]  [(nan, NN)]  \n",
       "3  [[(n, NN)], [(a, DT)], [(n, NN)]]  [(nan, NN)]  \n",
       "4  [[(n, NN)], [(a, DT)], [(n, NN)]]  [(nan, NN)]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>com_token</th>\n",
       "      <th>com_tagged</th>\n",
       "      <th>tagged</th>\n",
       "      <th>com_remv</th>\n",
       "      <th>com_lemma</th>\n",
       "      <th>com_stem</th>\n",
       "      <th>com_full</th>\n",
       "      <th>com_stem_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>omgidk y but i just cant stop laughinglmao</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[(n, NN)], [(a, DT)], [(n, NN)]]</td>\n",
       "      <td>[(nan, NN)]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There is literally not one thing about Krystal...</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[(n, NN)], [(a, DT)], [(n, NN)]]</td>\n",
       "      <td>[(nan, NN)]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[(n, NN)], [(a, DT)], [(n, NN)]]</td>\n",
       "      <td>[(nan, NN)]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Try not to laugh challenge right there</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[(n, NN)], [(a, DT)], [(n, NN)]]</td>\n",
       "      <td>[(nan, NN)]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>httpswwwyoutubecomwatchvKtI40e1cEN4</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[(n, NN)], [(a, DT)], [(n, NN)]]</td>\n",
       "      <td>[(nan, NN)]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label comment com_token  \\\n",
       "0        omgidk y but i just cant stop laughinglmao      nan     [nan]   \n",
       "1  There is literally not one thing about Krystal...     nan     [nan]   \n",
       "2                                                        nan     [nan]   \n",
       "3            Try not to laugh challenge right there      nan     [nan]   \n",
       "4               httpswwwyoutubecomwatchvKtI40e1cEN4      nan     [nan]   \n",
       "\n",
       "                          com_tagged       tagged com_remv com_lemma com_stem  \\\n",
       "0  [[(n, NN)], [(a, DT)], [(n, NN)]]  [(nan, NN)]    [nan]     [nan]    [nan]   \n",
       "1  [[(n, NN)], [(a, DT)], [(n, NN)]]  [(nan, NN)]    [nan]     [nan]    [nan]   \n",
       "2  [[(n, NN)], [(a, DT)], [(n, NN)]]  [(nan, NN)]    [nan]     [nan]    [nan]   \n",
       "3  [[(n, NN)], [(a, DT)], [(n, NN)]]  [(nan, NN)]    [nan]     [nan]    [nan]   \n",
       "4  [[(n, NN)], [(a, DT)], [(n, NN)]]  [(nan, NN)]    [nan]     [nan]    [nan]   \n",
       "\n",
       "  com_full com_stem_str  \n",
       "0      nan          nan  \n",
       "1      nan          nan  \n",
       "2      nan          nan  \n",
       "3      nan          nan  \n",
       "4      nan          nan  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = nlpFunction(df)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(DataList)): \n",
    "    nlpFunction(DataList[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(excluded)): \n",
    "    nlpFunction(excluded[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nlpFunction(df)\n",
    "data = nlpFunction(data)\n",
    "trump = nlpFunction(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>com_token</th>\n",
       "      <th>com_remv</th>\n",
       "      <th>com_lemma</th>\n",
       "      <th>com_stem</th>\n",
       "      <th>com_full</th>\n",
       "      <th>com_tagged</th>\n",
       "      <th>com_stem_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>Everyone knows brand s papers from  But  No on...</td>\n",
       "      <td>[everyone, knows, brand, s, papers, from, but,...</td>\n",
       "      <td>[everyone, knows, brand, papers, one, knows, w...</td>\n",
       "      <td>[everyone, know, brand, paper, one, know, welf...</td>\n",
       "      <td>[everyon, know, brand, paper, one, know, welfa...</td>\n",
       "      <td>everyon know brand paper one know welfar emplo...</td>\n",
       "      <td>[[(E, NN)], [(v, NN)], [(e, NN)], [(r, NN)], [...</td>\n",
       "      <td>everyon, know, brand, paper, one, know, welfar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Your paper cut balance is</td>\n",
       "      <td>[your, paper, cut, balance, is]</td>\n",
       "      <td>[paper, cut, balance]</td>\n",
       "      <td>[paper, cut, balance]</td>\n",
       "      <td>[paper, cut, balanc]</td>\n",
       "      <td>paper cut balanc</td>\n",
       "      <td>[[( , NN)], [(Y, NN)], [(o, NN)], [(u, NN)], [...</td>\n",
       "      <td>paper, cut, balanc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>OH SHIT WHEN I SAW THIS ON MY FRONT PAGE      ...</td>\n",
       "      <td>[oh, shit, when, i, saw, this, on, my, front, ...</td>\n",
       "      <td>[oh, shit, saw, front, page, love, song]</td>\n",
       "      <td>[oh, shit, saw, front, page, love, song]</td>\n",
       "      <td>[oh, shit, saw, front, page, love, song]</td>\n",
       "      <td>oh shit saw front page love song</td>\n",
       "      <td>[[(O, NN)], [(H, NN)], [( , NN)], [(S, NN)], [...</td>\n",
       "      <td>oh, shit, saw, front, page, love, song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Blowing my mind yet again</td>\n",
       "      <td>[blowing, my, mind, yet, again]</td>\n",
       "      <td>[blowing, mind, yet]</td>\n",
       "      <td>[blowing, mind, yet]</td>\n",
       "      <td>[blow, mind, yet]</td>\n",
       "      <td>blow mind yet</td>\n",
       "      <td>[[(B, NN)], [(l, NN)], [(o, NN)], [(w, NN)], [...</td>\n",
       "      <td>blow, mind, yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Should have gone with Dunder Mifflin</td>\n",
       "      <td>[should, have, gone, with, dunder, mifflin]</td>\n",
       "      <td>[gone, dunder, mifflin]</td>\n",
       "      <td>[gone, dunder, mifflin]</td>\n",
       "      <td>[gone, dunder, mifflin]</td>\n",
       "      <td>gone dunder mifflin</td>\n",
       "      <td>[[(S, NN)], [(h, NN)], [(o, NN)], [(u, NN)], [...</td>\n",
       "      <td>gone, dunder, mifflin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0   -1.0  Everyone knows brand s papers from  But  No on...   \n",
       "1    0.0         Your paper cut balance is                    \n",
       "2    1.0  OH SHIT WHEN I SAW THIS ON MY FRONT PAGE      ...   \n",
       "3    1.0                          Blowing my mind yet again   \n",
       "4    0.0               Should have gone with Dunder Mifflin   \n",
       "\n",
       "                                           com_token  \\\n",
       "0  [everyone, knows, brand, s, papers, from, but,...   \n",
       "1                    [your, paper, cut, balance, is]   \n",
       "2  [oh, shit, when, i, saw, this, on, my, front, ...   \n",
       "3                    [blowing, my, mind, yet, again]   \n",
       "4        [should, have, gone, with, dunder, mifflin]   \n",
       "\n",
       "                                            com_remv  \\\n",
       "0  [everyone, knows, brand, papers, one, knows, w...   \n",
       "1                              [paper, cut, balance]   \n",
       "2           [oh, shit, saw, front, page, love, song]   \n",
       "3                               [blowing, mind, yet]   \n",
       "4                            [gone, dunder, mifflin]   \n",
       "\n",
       "                                           com_lemma  \\\n",
       "0  [everyone, know, brand, paper, one, know, welf...   \n",
       "1                              [paper, cut, balance]   \n",
       "2           [oh, shit, saw, front, page, love, song]   \n",
       "3                               [blowing, mind, yet]   \n",
       "4                            [gone, dunder, mifflin]   \n",
       "\n",
       "                                            com_stem  \\\n",
       "0  [everyon, know, brand, paper, one, know, welfa...   \n",
       "1                               [paper, cut, balanc]   \n",
       "2           [oh, shit, saw, front, page, love, song]   \n",
       "3                                  [blow, mind, yet]   \n",
       "4                            [gone, dunder, mifflin]   \n",
       "\n",
       "                                            com_full  \\\n",
       "0  everyon know brand paper one know welfar emplo...   \n",
       "1                                   paper cut balanc   \n",
       "2                   oh shit saw front page love song   \n",
       "3                                      blow mind yet   \n",
       "4                                gone dunder mifflin   \n",
       "\n",
       "                                          com_tagged  \\\n",
       "0  [[(E, NN)], [(v, NN)], [(e, NN)], [(r, NN)], [...   \n",
       "1  [[( , NN)], [(Y, NN)], [(o, NN)], [(u, NN)], [...   \n",
       "2  [[(O, NN)], [(H, NN)], [( , NN)], [(S, NN)], [...   \n",
       "3  [[(B, NN)], [(l, NN)], [(o, NN)], [(w, NN)], [...   \n",
       "4  [[(S, NN)], [(h, NN)], [(o, NN)], [(u, NN)], [...   \n",
       "\n",
       "                                        com_stem_str  \n",
       "0  everyon, know, brand, paper, one, know, welfar...  \n",
       "1                                 paper, cut, balanc  \n",
       "2             oh, shit, saw, front, page, love, song  \n",
       "3                                    blow, mind, yet  \n",
       "4                              gone, dunder, mifflin  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import itertools\\nfrom nltk.collocations import BigramCollocationFinder\\nfrom nltk.metrics import BigramAssocMeasures\\n \\ndef bigram_word_feats(DF, score_fn=BigramAssocMeasures.chi_sq, n=200):\\n    bigram_finder = BigramCollocationFinder.from_words(DF[\"\"])\\n    bigrams = bigram_finder.nbest(score_fn, n)\\n    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\\n \\nevaluate_classifier(bigram_word_feats)'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    " \n",
    "def bigram_word_feats(DF, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(DF[\"\"])\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    " \n",
    "evaluate_classifier(bigram_word_feats)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER (Named Entry Recognition) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from nltk.tag import StanfordNERTagger\\nfrom nltk.tokenize import word_tokenize\\n\\nst = StanfordNERTagger(\\'/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz\\',\\n                       \\'/usr/share/stanford-ner/stanford-ner.jar\\',\\n                       encoding=\\'utf-8\\')\\n\\ntext = df[\"comments\"]\\n\\ntok_text = word_tokenize(text)\\nst_text = st.tag(tokenized_text)\\n\\nprint(st_text)'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                       '/usr/share/stanford-ner/stanford-ner.jar',\n",
    "                       encoding='utf-8')\n",
    "\n",
    "text = df[\"comments\"]\n",
    "\n",
    "tok_text = word_tokenize(text)\n",
    "st_text = st.tag(tokenized_text)\n",
    "\n",
    "print(st_text)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text\n",
    "from nltk import everygrams\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from nltk import everygrams, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['String'].apply(lambda x: [' '.join(ng) for ng in everygrams(word_tokenize(x), 1, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams and Trigrams\n",
    "https://stackoverflow.com/questions/49147128/generate-ngrams-from-a-pandas-dataframe-column\n",
    "https://stackoverflow.com/questions/32252075/nltk-sklearn-unigram-bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i = videos.comment      .str.lower()      .str.replace('[^a-z\\\\s]', '')      .str.split(expand=True)      .stack()\\n\\n# generate bigrams by concatenating unigram columns\\nj = i + ' ' + i.shift(-1)\\n# generate trigrams by concatenating unigram and bigram columns\\nk = j + ' ' + i.shift(-2)\\n\\n# concatenate all series vertically, and remove NaNs\\npd.concat([i, j, k]).dropna().reset_index(drop=True)\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''i = videos.comment\\\n",
    "      .str.lower()\\\n",
    "      .str.replace('[^a-z\\s]', '')\\\n",
    "      .str.split(expand=True)\\\n",
    "      .stack()\n",
    "\n",
    "# generate bigrams by concatenating unigram columns\n",
    "j = i + ' ' + i.shift(-1)\n",
    "# generate trigrams by concatenating unigram and bigram columns\n",
    "k = j + ' ' + i.shift(-2)\n",
    "\n",
    "# concatenate all series vertically, and remove NaNs\n",
    "pd.concat([i, j, k]).dropna().reset_index(drop=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#videos['comment'].apply(lambda x: [' '.join(ng) for ng in everygrams(word_tokenize(x), 1, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ngram_size = 2\\nvectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(ngram_size,ngram_size))\\n\\nvectorizer.fit(videos[\"com_full\"]) # build ngram dictionary\\nngram = vectorizer.transform(videos[\"com_full\"]) # get ngram\\nprint(\\'ngram: {0}\\n\\'.format(ngram))\\nprint(\\'ngram.shape: {0}\\'.format(ngram.shape))\\nprint(\\'vectorizer.vocabulary_: {0}\\'.format(vectorizer.vocabulary_))'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''ngram_size = 2\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(ngram_size,ngram_size))\n",
    "\n",
    "vectorizer.fit(videos[\"com_full\"]) # build ngram dictionary\n",
    "ngram = vectorizer.transform(videos[\"com_full\"]) # get ngram\n",
    "print('ngram: {0}\\n'.format(ngram))\n",
    "print('ngram.shape: {0}'.format(ngram.shape))\n",
    "print('vectorizer.vocabulary_: {0}'.format(vectorizer.vocabulary_))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bigrams(input_list):\n",
    "  bigram_list = []\n",
    "  for i in range(len(input_list)-1):\n",
    "      bigram_list.append((input_list[i], input_list[i+1]))\n",
    "  return bigram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos['bigrams'] = videos['com_stem'].apply(lambda x: find_bigrams(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>com_token</th>\n",
       "      <th>com_remv</th>\n",
       "      <th>com_lemma</th>\n",
       "      <th>com_stem</th>\n",
       "      <th>com_full</th>\n",
       "      <th>com_tagged</th>\n",
       "      <th>com_stem_str</th>\n",
       "      <th>bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>Everyone knows brand s papers from  But  No on...</td>\n",
       "      <td>[everyone, knows, brand, s, papers, from, but,...</td>\n",
       "      <td>[everyone, knows, brand, papers, one, knows, w...</td>\n",
       "      <td>[everyone, know, brand, paper, one, know, welf...</td>\n",
       "      <td>[everyon, know, brand, paper, one, know, welfa...</td>\n",
       "      <td>everyon know brand paper one know welfar emplo...</td>\n",
       "      <td>[[(E, NN)], [(v, NN)], [(e, NN)], [(r, NN)], [...</td>\n",
       "      <td>everyon, know, brand, paper, one, know, welfar...</td>\n",
       "      <td>[(everyon, know), (know, brand), (brand, paper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Your paper cut balance is</td>\n",
       "      <td>[your, paper, cut, balance, is]</td>\n",
       "      <td>[paper, cut, balance]</td>\n",
       "      <td>[paper, cut, balance]</td>\n",
       "      <td>[paper, cut, balanc]</td>\n",
       "      <td>paper cut balanc</td>\n",
       "      <td>[[( , NN)], [(Y, NN)], [(o, NN)], [(u, NN)], [...</td>\n",
       "      <td>paper, cut, balanc</td>\n",
       "      <td>[(paper, cut), (cut, balanc)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0   -1.0  Everyone knows brand s papers from  But  No on...   \n",
       "1    0.0         Your paper cut balance is                    \n",
       "\n",
       "                                           com_token  \\\n",
       "0  [everyone, knows, brand, s, papers, from, but,...   \n",
       "1                    [your, paper, cut, balance, is]   \n",
       "\n",
       "                                            com_remv  \\\n",
       "0  [everyone, knows, brand, papers, one, knows, w...   \n",
       "1                              [paper, cut, balance]   \n",
       "\n",
       "                                           com_lemma  \\\n",
       "0  [everyone, know, brand, paper, one, know, welf...   \n",
       "1                              [paper, cut, balance]   \n",
       "\n",
       "                                            com_stem  \\\n",
       "0  [everyon, know, brand, paper, one, know, welfa...   \n",
       "1                               [paper, cut, balanc]   \n",
       "\n",
       "                                            com_full  \\\n",
       "0  everyon know brand paper one know welfar emplo...   \n",
       "1                                   paper cut balanc   \n",
       "\n",
       "                                          com_tagged  \\\n",
       "0  [[(E, NN)], [(v, NN)], [(e, NN)], [(r, NN)], [...   \n",
       "1  [[( , NN)], [(Y, NN)], [(o, NN)], [(u, NN)], [...   \n",
       "\n",
       "                                        com_stem_str  \\\n",
       "0  everyon, know, brand, paper, one, know, welfar...   \n",
       "1                                 paper, cut, balanc   \n",
       "\n",
       "                                             bigrams  \n",
       "0  [(everyon, know), (know, brand), (brand, paper...  \n",
       "1                      [(paper, cut), (cut, balanc)]  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penalized Regressions\n",
    "https://stackoverflow.com/questions/12247768/unigrams-bigrams-tf-idf-less-accurate-than-just-unigrams-ff-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.linear_model.Lasso (regression)\n",
    "sklearn.linear_model.ElasticNet (regression)\n",
    "sklearn.linear_model.SGDRegressor (regression) with penalty == 'elastic_net' or 'l1'\n",
    "sklearn.linear_model.SGDClassifier (classification) with penalty == 'elastic_net' or 'l1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['String'].apply(lambda x: [' '.join(ng) for ng in everygrams(word_tokenize(x), 1, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ngram_size = 1\n",
    "train_set = ['Cristiano plays football', 'Ronaldo like football too']\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(ngram_size,ngram_size))\n",
    "vectorizer.fit(train_set) # build ngram dictionary\n",
    "ngram = vectorizer.transform(train_set) # get ngram'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Split into Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn # machine learning\n",
    "from sklearn.model_selection import train_test_split # splitting up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[\"com_stem_str\"]\n",
    "X_test = trump[\"com_stem_str\"]\n",
    "Y_train = data[\"label\"]\n",
    "Y_test = trump[\"label\"]\n",
    "X_user = df[\"com_stem_str\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = videos.dropna(axis=0)\n",
    "videos_not_okgo = videos_not_okgo.dropna(axis=0)\n",
    "videos_not_royal = videos_not_royal.dropna(axis=0)\n",
    "royal = royal.dropna(axis=0)\n",
    "okgo = okgo.dropna(axis=0)\n",
    "full = full.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_videos, x_test_videos, y_train_videos, y_test_videos = train_test_split(\n",
    "    videos[\"com_stem_str\"], videos[\"label\"], test_size=0.33, random_state=42)\n",
    "\n",
    "x_train_full, x_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    full[\"com_stem_str\"], full[\"label\"], test_size=0.33, random_state=42)\n",
    "\n",
    "####\n",
    "x_train_not_okgo = videos_not_okgo[\"com_stem_str\"]\n",
    "x_test_okgo = okgo[\"com_stem_str\"]\n",
    "\n",
    "y_train_not_okgo = videos_not_okgo[\"label\"]\n",
    "y_test_okgo = okgo[\"label\"]\n",
    "\n",
    "####\n",
    "x_train_not_royal = videos_not_royal[\"com_stem_str\"]\n",
    "x_test_royal = royal[\"com_stem_str\"]\n",
    "\n",
    "y_train_not_royal = videos_not_royal[\"label\"]\n",
    "y_test_royal = royal[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>com_token</th>\n",
       "      <th>com_remv</th>\n",
       "      <th>com_lemma</th>\n",
       "      <th>com_stem</th>\n",
       "      <th>com_full</th>\n",
       "      <th>com_tagged</th>\n",
       "      <th>com_stem_str</th>\n",
       "      <th>bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>Everyone knows brand s papers from  But  No on...</td>\n",
       "      <td>[everyone, knows, brand, s, papers, from, but,...</td>\n",
       "      <td>[everyone, knows, brand, papers, one, knows, w...</td>\n",
       "      <td>[everyone, know, brand, paper, one, know, welf...</td>\n",
       "      <td>[everyon, know, brand, paper, one, know, welfa...</td>\n",
       "      <td>everyon know brand paper one know welfar emplo...</td>\n",
       "      <td>[[(E, NN)], [(v, NN)], [(e, NN)], [(r, NN)], [...</td>\n",
       "      <td>everyon, know, brand, paper, one, know, welfar...</td>\n",
       "      <td>[(everyon, know), (know, brand), (brand, paper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Your paper cut balance is</td>\n",
       "      <td>[your, paper, cut, balance, is]</td>\n",
       "      <td>[paper, cut, balance]</td>\n",
       "      <td>[paper, cut, balance]</td>\n",
       "      <td>[paper, cut, balanc]</td>\n",
       "      <td>paper cut balanc</td>\n",
       "      <td>[[( , NN)], [(Y, NN)], [(o, NN)], [(u, NN)], [...</td>\n",
       "      <td>paper, cut, balanc</td>\n",
       "      <td>[(paper, cut), (cut, balanc)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>OH SHIT WHEN I SAW THIS ON MY FRONT PAGE      ...</td>\n",
       "      <td>[oh, shit, when, i, saw, this, on, my, front, ...</td>\n",
       "      <td>[oh, shit, saw, front, page, love, song]</td>\n",
       "      <td>[oh, shit, saw, front, page, love, song]</td>\n",
       "      <td>[oh, shit, saw, front, page, love, song]</td>\n",
       "      <td>oh shit saw front page love song</td>\n",
       "      <td>[[(O, NN)], [(H, NN)], [( , NN)], [(S, NN)], [...</td>\n",
       "      <td>oh, shit, saw, front, page, love, song</td>\n",
       "      <td>[(oh, shit), (shit, saw), (saw, front), (front...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0   -1.0  Everyone knows brand s papers from  But  No on...   \n",
       "1    0.0         Your paper cut balance is                    \n",
       "2    1.0  OH SHIT WHEN I SAW THIS ON MY FRONT PAGE      ...   \n",
       "\n",
       "                                           com_token  \\\n",
       "0  [everyone, knows, brand, s, papers, from, but,...   \n",
       "1                    [your, paper, cut, balance, is]   \n",
       "2  [oh, shit, when, i, saw, this, on, my, front, ...   \n",
       "\n",
       "                                            com_remv  \\\n",
       "0  [everyone, knows, brand, papers, one, knows, w...   \n",
       "1                              [paper, cut, balance]   \n",
       "2           [oh, shit, saw, front, page, love, song]   \n",
       "\n",
       "                                           com_lemma  \\\n",
       "0  [everyone, know, brand, paper, one, know, welf...   \n",
       "1                              [paper, cut, balance]   \n",
       "2           [oh, shit, saw, front, page, love, song]   \n",
       "\n",
       "                                            com_stem  \\\n",
       "0  [everyon, know, brand, paper, one, know, welfa...   \n",
       "1                               [paper, cut, balanc]   \n",
       "2           [oh, shit, saw, front, page, love, song]   \n",
       "\n",
       "                                            com_full  \\\n",
       "0  everyon know brand paper one know welfar emplo...   \n",
       "1                                   paper cut balanc   \n",
       "2                   oh shit saw front page love song   \n",
       "\n",
       "                                          com_tagged  \\\n",
       "0  [[(E, NN)], [(v, NN)], [(e, NN)], [(r, NN)], [...   \n",
       "1  [[( , NN)], [(Y, NN)], [(o, NN)], [(u, NN)], [...   \n",
       "2  [[(O, NN)], [(H, NN)], [( , NN)], [(S, NN)], [...   \n",
       "\n",
       "                                        com_stem_str  \\\n",
       "0  everyon, know, brand, paper, one, know, welfar...   \n",
       "1                                 paper, cut, balanc   \n",
       "2             oh, shit, saw, front, page, love, song   \n",
       "\n",
       "                                             bigrams  \n",
       "0  [(everyon, know), (know, brand), (brand, paper...  \n",
       "1                      [(paper, cut), (cut, balanc)]  \n",
       "2  [(oh, shit), (shit, saw), (saw, front), (front...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#videos[\"bigrams_str\"] = videos[\"bigrams\"].str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#videos[\"bigrams_str\"]  = videos[\"bigrams\"].apply(lambda x: ', '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = videos.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_videos_bi_tk, x_test_videos_bi_tk, y_train_videos_bi, y_test_videos_bi = train_test_split(\n",
    "    videos[\"com_full\"], videos[\"label\"], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bi_CV = CountVectorizer(ngram_range=(1, 2))\n",
    "x_train_videos_bi = bi_CV.fit_transform(x_train_videos_bi_tk) # transform and fit training data\n",
    "x_test_videos_bi = bi_CV.transform(x_test_videos_bi_tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Miss(X_train, Y_train, X_test, Y_test):\n",
    "    print('lengths training variables: ', len(X_train),\",\", len(Y_train))\n",
    "    print('lengths testing variables: ', len(X_test),\",\", len(Y_test), '\\n')\n",
    "\n",
    "    print('Are there any missing values?', \n",
    "          '\\n * Training:', pd.isnull(X_train).values.any(), ',', pd.isnull(Y_train).values.any(), \n",
    "          '\\n * Testing: ', pd.isnull(X_test).values.any(), \",\", pd.isnull(Y_test).values.any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengths training variables:  1763 , 1763\n",
      "lengths testing variables:  869 , 869 \n",
      "\n",
      "Are there any missing values? \n",
      " * Training: False , False \n",
      " * Testing:  False , False\n"
     ]
    }
   ],
   "source": [
    "Miss(x_train_videos_bi_tk, y_train_videos_bi, x_test_videos_bi_tk, y_test_videos_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Transform Data to Counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>com_token</th>\n",
       "      <th>com_remv</th>\n",
       "      <th>com_lemma</th>\n",
       "      <th>com_stem</th>\n",
       "      <th>com_full</th>\n",
       "      <th>com_tagged</th>\n",
       "      <th>com_stem_str</th>\n",
       "      <th>bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>Everyone knows brand s papers from  But  No on...</td>\n",
       "      <td>[everyone, knows, brand, s, papers, from, but,...</td>\n",
       "      <td>[everyone, knows, brand, papers, one, knows, w...</td>\n",
       "      <td>[everyone, know, brand, paper, one, know, welf...</td>\n",
       "      <td>[everyon, know, brand, paper, one, know, welfa...</td>\n",
       "      <td>everyon know brand paper one know welfar emplo...</td>\n",
       "      <td>[[(E, NN)], [(v, NN)], [(e, NN)], [(r, NN)], [...</td>\n",
       "      <td>everyon, know, brand, paper, one, know, welfar...</td>\n",
       "      <td>[(everyon, know), (know, brand), (brand, paper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Your paper cut balance is</td>\n",
       "      <td>[your, paper, cut, balance, is]</td>\n",
       "      <td>[paper, cut, balance]</td>\n",
       "      <td>[paper, cut, balance]</td>\n",
       "      <td>[paper, cut, balanc]</td>\n",
       "      <td>paper cut balanc</td>\n",
       "      <td>[[( , NN)], [(Y, NN)], [(o, NN)], [(u, NN)], [...</td>\n",
       "      <td>paper, cut, balanc</td>\n",
       "      <td>[(paper, cut), (cut, balanc)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>OH SHIT WHEN I SAW THIS ON MY FRONT PAGE      ...</td>\n",
       "      <td>[oh, shit, when, i, saw, this, on, my, front, ...</td>\n",
       "      <td>[oh, shit, saw, front, page, love, song]</td>\n",
       "      <td>[oh, shit, saw, front, page, love, song]</td>\n",
       "      <td>[oh, shit, saw, front, page, love, song]</td>\n",
       "      <td>oh shit saw front page love song</td>\n",
       "      <td>[[(O, NN)], [(H, NN)], [( , NN)], [(S, NN)], [...</td>\n",
       "      <td>oh, shit, saw, front, page, love, song</td>\n",
       "      <td>[(oh, shit), (shit, saw), (saw, front), (front...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0   -1.0  Everyone knows brand s papers from  But  No on...   \n",
       "1    0.0         Your paper cut balance is                    \n",
       "2    1.0  OH SHIT WHEN I SAW THIS ON MY FRONT PAGE      ...   \n",
       "\n",
       "                                           com_token  \\\n",
       "0  [everyone, knows, brand, s, papers, from, but,...   \n",
       "1                    [your, paper, cut, balance, is]   \n",
       "2  [oh, shit, when, i, saw, this, on, my, front, ...   \n",
       "\n",
       "                                            com_remv  \\\n",
       "0  [everyone, knows, brand, papers, one, knows, w...   \n",
       "1                              [paper, cut, balance]   \n",
       "2           [oh, shit, saw, front, page, love, song]   \n",
       "\n",
       "                                           com_lemma  \\\n",
       "0  [everyone, know, brand, paper, one, know, welf...   \n",
       "1                              [paper, cut, balance]   \n",
       "2           [oh, shit, saw, front, page, love, song]   \n",
       "\n",
       "                                            com_stem  \\\n",
       "0  [everyon, know, brand, paper, one, know, welfa...   \n",
       "1                               [paper, cut, balanc]   \n",
       "2           [oh, shit, saw, front, page, love, song]   \n",
       "\n",
       "                                            com_full  \\\n",
       "0  everyon know brand paper one know welfar emplo...   \n",
       "1                                   paper cut balanc   \n",
       "2                   oh shit saw front page love song   \n",
       "\n",
       "                                          com_tagged  \\\n",
       "0  [[(E, NN)], [(v, NN)], [(e, NN)], [(r, NN)], [...   \n",
       "1  [[( , NN)], [(Y, NN)], [(o, NN)], [(u, NN)], [...   \n",
       "2  [[(O, NN)], [(H, NN)], [( , NN)], [(S, NN)], [...   \n",
       "\n",
       "                                        com_stem_str  \\\n",
       "0  everyon, know, brand, paper, one, know, welfar...   \n",
       "1                                 paper, cut, balanc   \n",
       "2             oh, shit, saw, front, page, love, song   \n",
       "\n",
       "                                             bigrams  \n",
       "0  [(everyon, know), (know, brand), (brand, paper...  \n",
       "1                      [(paper, cut), (cut, balanc)]  \n",
       "2  [(oh, shit), (shit, saw), (saw, front), (front...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "xtrain = tfidf.fit_transform(X_train) # transform and fit training data\n",
    "xtest = tfidf.transform(X_test) # transform trump test data from fitted transformer\n",
    "xuser = tfidf.transform(X_user) # transform user selected comments to predict on\n",
    "\n",
    "data_trans= tfidf.transform(data[\"com_stem_str\"]) # same as X_train...transform entire dataset for cross validation\n",
    "df_trans = tfidf.transform(df[\"com_stem_str\"]) # same as X_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_videos = tfidf.fit_transform(x_train_videos)\n",
    "x_test_videos = tfidf.transform(x_test_videos)\n",
    "\n",
    "x_train_full = tfidf.fit_transform(x_train_full)\n",
    "x_test_full = tfidf.transform(x_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_not_okgo = tfidf.fit_transform(x_train_not_okgo)\n",
    "x_test_okgo = tfidf.transform(x_test_okgo)\n",
    "\n",
    "####\n",
    "x_train_not_royal = tfidf.fit_transform(x_train_not_royal)\n",
    "x_test_royal = tfidf.transform(x_test_royal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tagged = tfidf.transform(df[\"tagged\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm # support vector machine\n",
    "from sklearn import metrics # for accuracy/ precision\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier # Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "lr = LogisticRegression(solver='sag', max_iter=100, random_state=42, multi_class=\"multinomial\")\n",
    "#svm = svm.SVC()\n",
    "knn = KNeighborsClassifier()\n",
    "xgb = XGBClassifier()\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x_train_videos = tfidf.fit_transform(x_train_videos)\\nx_test_videos = tfidf.transform(x_test_videos)\\n\\nx_train_full = tfidf.fit_transform(x_train_full)\\nx_test_full = tfidf.transform(x_test_full)\\n\\n####\\nx_train_not_okgo = tfidf.fit_transform(x_train_not_okgo)\\nx_test_not_okgo = tfidf.transform(x_test_not_okgo)\\n\\n####\\nx_train_not_royal = tfidf.fit_transform(x_train_not_royal)\\nx_test_not_royal = tfidf.transform(x_test_not_royal)\\n'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''x_train_videos = tfidf.fit_transform(x_train_videos)\n",
    "x_test_videos = tfidf.transform(x_test_videos)\n",
    "\n",
    "x_train_full = tfidf.fit_transform(x_train_full)\n",
    "x_test_full = tfidf.transform(x_test_full)\n",
    "\n",
    "####\n",
    "x_train_not_okgo = tfidf.fit_transform(x_train_not_okgo)\n",
    "x_test_not_okgo = tfidf.transform(x_test_not_okgo)\n",
    "\n",
    "####\n",
    "x_train_not_royal = tfidf.fit_transform(x_train_not_royal)\n",
    "x_test_not_royal = tfidf.transform(x_test_not_royal)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLpipeline(model, xtrain, xtest, ytrain, ytest):\n",
    "    model.fit(xtrain, ytrain)\n",
    "    model_predict = model.predict(xtest)\n",
    "    model_acc = metrics.accuracy_score(ytest, model_predict)\n",
    "    print('We obtained ', round(model_acc, 6), '% accuracy for the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained  0.621404 % accuracy for the model\n",
      "We obtained  0.623705 % accuracy for the model\n",
      "We obtained  0.56962 % accuracy for the model\n",
      "We obtained  0.593786 % accuracy for the model\n",
      "We obtained  0.621404 % accuracy for the model\n"
     ]
    }
   ],
   "source": [
    "MLpipeline(mnb, x_train_videos_bi, x_test_videos_bi, y_train_videos_bi, y_test_videos_bi)\n",
    "MLpipeline(lr, x_train_videos_bi, x_test_videos_bi, y_train_videos_bi, y_test_videos_bi)\n",
    "MLpipeline(knn, x_train_videos_bi, x_test_videos_bi, y_train_videos_bi, y_test_videos_bi)\n",
    "MLpipeline(xgb, x_train_videos_bi, x_test_videos_bi, y_train_videos_bi, y_test_videos_bi)\n",
    "MLpipeline(rf, x_train_videos_bi, x_test_videos_bi, y_train_videos_bi, y_test_videos_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained  0.629459 % accuracy for the model\n",
      "We obtained  0.63061 % accuracy for the model\n",
      "We obtained  0.551208 % accuracy for the model\n",
      "We obtained  0.594937 % accuracy for the model\n",
      "We obtained  0.614499 % accuracy for the model\n"
     ]
    }
   ],
   "source": [
    "MLpipeline(mnb, x_train_videos, x_test_videos, y_train_videos, y_test_videos)\n",
    "MLpipeline(lr, x_train_videos, x_test_videos, y_train_videos, y_test_videos)\n",
    "MLpipeline(knn, x_train_videos, x_test_videos, y_train_videos, y_test_videos)\n",
    "MLpipeline(xgb, x_train_videos, x_test_videos, y_train_videos, y_test_videos)\n",
    "MLpipeline(rf, x_train_videos, x_test_videos, y_train_videos, y_test_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained  0.500502 % accuracy for the model\n",
      "We obtained  0.512036 % accuracy for the model\n",
      "We obtained  0.461886 % accuracy for the model\n",
      "We obtained  0.519559 % accuracy for the model\n",
      "We obtained  0.535105 % accuracy for the model\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d = x_train_not_okgo, x_test_okgo, y_train_not_okgo, y_test_okgo\n",
    "\n",
    "MLpipeline(mnb, a, b, c, d)\n",
    "MLpipeline(lr, a, b, c, d)\n",
    "MLpipeline(knn, a, b, c, d)\n",
    "MLpipeline(xgb, a, b, c, d)\n",
    "MLpipeline(rf, a, b, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained  0.837965 % accuracy for the model\n",
      "We obtained  0.870045 % accuracy for the model\n",
      "We obtained  0.820597 % accuracy for the model\n",
      "We obtained  0.847568 % accuracy for the model\n",
      "We obtained  0.861872 % accuracy for the model\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d = x_train_full, x_test_full, y_train_full, y_test_full\n",
    "\n",
    "MLpipeline(mnb, a, b, c, d)\n",
    "MLpipeline(lr, a, b, c, d)\n",
    "MLpipeline(knn, a, b, c, d)\n",
    "MLpipeline(xgb, a, b, c, d)\n",
    "MLpipeline(rf, a, b, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained  0.655738 % accuracy for the model\n",
      "We obtained  0.639344 % accuracy for the model\n",
      "We obtained  0.540984 % accuracy for the model\n",
      "We obtained  0.639344 % accuracy for the model\n",
      "We obtained  0.655738 % accuracy for the model\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d = x_train_not_royal, x_test_royal, y_train_not_royal, y_test_royal\n",
    "\n",
    "MLpipeline(mnb, a, b, c, d)\n",
    "MLpipeline(lr, a, b, c, d)\n",
    "MLpipeline(knn, a, b, c, d)\n",
    "MLpipeline(xgb, a, b, c, d)\n",
    "MLpipeline(rf, a, b, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Multinomial Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(xtrain, Y_train) # fit the model on the training data word counts and training data lables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Predictions:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained  0.477387 % accuracy for the model\n"
     ]
    }
   ],
   "source": [
    "mnb_predict = mnb.predict(xtest) # make our y predictions (labels) on the comment test data\n",
    "mnb_acc = metrics.accuracy_score(Y_test, mnb_predict)\n",
    "print('We obtained ', round(mnb_acc, 6), '% accuracy for the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.00      0.00      0.00        56\n",
      "          0       0.49      0.89      0.63        87\n",
      "          1       0.44      0.32      0.37        56\n",
      "\n",
      "avg / total       0.34      0.48      0.38       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(Y_test, mnb_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 43, 13],\n",
       "       [ 0, 77, 10],\n",
       "       [ 0, 38, 18]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Y_test, mnb_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation of Accuracy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Interval for Accuracy: 0.59 (+/- 0.11)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(mnb, xtest, Y_test, cv=5) # 5 fold cross validation\n",
    "print(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='sag', max_iter=100, random_state=42, multi_class=\"multinomial\") # set multinomial setting for multiclass data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=42, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(xtrain, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained  0.507538 % accuracy for the logistic regression model\n"
     ]
    }
   ],
   "source": [
    "lr_predict = lr.predict(xtest)\n",
    "lr_acc = metrics.accuracy_score(Y_test, lr_predict)\n",
    "print('We obtained ', round(lr_acc, 6), '% accuracy for the logistic regression model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.75      0.05      0.10        56\n",
      "          0       0.48      0.92      0.63        87\n",
      "          1       0.64      0.32      0.43        56\n",
      "\n",
      "avg / total       0.60      0.51      0.42       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(Y_test, lr_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, 49,  4],\n",
       "       [ 1, 80,  6],\n",
       "       [ 0, 38, 18]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Y_test, lr_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Interval for Accuracy: 0.59 (+/- 0.16)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(lr, xtest, Y_test, cv=5) # 5 fold cross validation\n",
    "print(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the Model & Predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained  0.437186 % accuracy for the SVM model\n"
     ]
    }
   ],
   "source": [
    "svm = svm.SVC()\n",
    "svm.fit(xtrain, Y_train)\n",
    "svm_predict = svm.predict(xtest)\n",
    "svm_acc = metrics.accuracy_score(Y_test, svm_predict)\n",
    "print('We obtained ', round(svm_acc, 6), '% accuracy for the SVM model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.00      0.00      0.00        56\n",
      "          0       0.49      0.89      0.63        87\n",
      "          1       0.44      0.32      0.37        56\n",
      "\n",
      "avg / total       0.34      0.48      0.38       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(Y_test, mnb_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, 49,  4],\n",
       "       [ 1, 80,  6],\n",
       "       [ 0, 38, 18]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Y_test, lr_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Interval for Accuracy: 0.44 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(svm, xtest, Y_test, cv=5) # 5 fold cross validation\n",
    "print(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting Model & Predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained  0.432161 % accuracy for the KNN Bagging model\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # k-NN ensemble method\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(xtrain, Y_train)\n",
    "\n",
    "knn_predict = knn.predict(xtest)\n",
    "knn_acc = metrics.accuracy_score(Y_test, knn_predict)\n",
    "print('We obtained ', round(knn_acc, 6), '% accuracy for the KNN Bagging model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Interval for Accuracy: 0.38 (+/- 0.11)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(knn, xtest, Y_test, cv=5) # 5 fold cross validation\n",
    "print(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting Model & Predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained  0.537688 % accuracy for the Random Forest model\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier # random forest ensemble method\n",
    "\n",
    "ranfor = RandomForestClassifier(n_estimators=10, random_state=10)\n",
    "ranfor = ranfor.fit(xtrain, Y_train)\n",
    "\n",
    "rf_predict = ranfor.predict(xtest)\n",
    "rf_acc = metrics.accuracy_score(Y_test, rf_predict)\n",
    "print('We obtained ', round(rf_acc, 6), '% accuracy for the Random Forest model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Interval for Accuracy: 0.45 (+/- 0.19)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ranfor, xtest, Y_test, cv=5) # 5 fold cross validation\n",
    "print(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained  0.482412 % accuracy for the XGB Bagging model\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(xtrain, Y_train)\n",
    "xgb_pred = xgb.predict(xtest)\n",
    "xgb_acc = metrics.accuracy_score(Y_test, xgb_pred)\n",
    "print('We obtained ', round(xgb_acc, 6), '% accuracy for the XGB Bagging model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Interval for Accuracy: 0.57 (+/- 0.15)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(xgb, xtest, Y_test, cv=5) # 5 fold cross validation\n",
    "print(\"Confidence Interval for Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SavedData = open(\"pickled_material/data.pickle\",\"wb\")\n",
    "pickle.dump(documents, SavedData)\n",
    "SavedData.close()\n",
    "\n",
    "SavedFeatures = open(\"pickled_material/SavedFeatures.pickle\",\"wb\")\n",
    "pickle.dump(word_features, SavedFeatures)\n",
    "SavedFeatures.close()\n",
    "\n",
    "SavedModels= open(\"pickled_material/SavedModels.pickle\",\"wb\")\n",
    "pickle.dump(models, SavedModels)\n",
    "SavedModels.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Data Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Table of Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Support Vect Machine</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>K-NN</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.477387</td>\n",
       "      <td>0.437186</td>\n",
       "      <td>0.507538</td>\n",
       "      <td>0.432161</td>\n",
       "      <td>0.537688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Naive Bayes  Support Vect Machine  Logistic Regression      K-NN  \\\n",
       "Accuracy     0.477387              0.437186             0.507538  0.432161   \n",
       "\n",
       "          Random Forest  \n",
       "Accuracy       0.537688  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTable = pd.DataFrame(columns=['Naive Bayes','Support Vect Machine','Logistic Regression', 'K-NN', 'Random Forest'],\n",
    "                   index=[\"Accuracy\"])\n",
    "myTable['Naive Bayes']=mnb_acc; myTable['Support Vect Machine']=svm_acc; myTable['Logistic Regression']=lr_acc\n",
    "myTable['K-NN']= knn_acc; myTable['Random Forest']= rf_acc\n",
    "myTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
