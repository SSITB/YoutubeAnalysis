{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv; import numpy as np\n",
    "import re\n",
    "\n",
    "path = '/Users/andiedonovan/myProjects/Youtube_Python_Project/AndiesBranch/'\n",
    "os.chdir(path) # change directory\n",
    "\n",
    "# load in data\n",
    "\n",
    "# training data\n",
    "okgo = pd.read_csv('data/OKGO.csv', delimiter=\";\", skiprows=2, encoding='latin-1', engine='python') # read in the data\n",
    "trump = pd.read_csv('data/trump.csv', delimiter=\",\", skiprows=2, encoding='utf-8', error_bad_lines=False, engine='python')\n",
    "swift = pd.read_csv('data/TaylorSwift.csv', delimiter=\",\", skiprows=2, nrows=180, encoding='utf-8', engine='python')\n",
    "royal = pd.read_csv('data/RoyalWedding.csv', delimiter=\",\", skiprows=2, nrows=61, encoding='utf-8', engine='python')\n",
    "paul = pd.read_csv('data/LoganPaul.csv', delimiter=\",\", skiprows=2, nrows=200, encoding='utf-8', engine='python')\n",
    "blogs = pd.read_csv('data/Kagel.csv', delimiter=\",\", skiprows=2, encoding='latin-1', engine='python') # read in the data\n",
    "tweets = pd.read_csv('data/twitter.csv', delimiter=\",\", skiprows=2, encoding='latin-1', engine='python') # read in the data\n",
    "\n",
    "# test data:\n",
    "#trump = pd.read_csv('data/trump.csv', delimiter=\"@@@\", skiprows=2, encoding='utf-8', error_bad_lines=False, engine='python')\n",
    "# combine training dataframes\n",
    "df = pd.read_csv('data/data.csv', delimiter=\"@@@\", skiprows=2, encoding='utf-8', engine='python')\n",
    "\n",
    "# clean dataframes\n",
    "tweets = tweets.drop(['Topic', 'TweetId', \"TweetDate\"], axis = 1).dropna()\n",
    "\n",
    "def fix_cols(DF):\n",
    "    DF = DF.iloc[:,:2]\n",
    "    DF.columns = [\"label\", \"comment\"]\n",
    "    return DF\n",
    "\n",
    "okgo = fix_cols(okgo)\n",
    "trump = fix_cols(trump)\n",
    "swift = fix_cols(swift)\n",
    "royal = fix_cols(royal)\n",
    "paul = fix_cols(paul)\n",
    "tweets = fix_cols(tweets)\n",
    "\n",
    "tweets.label = tweets.label.replace({'positive': '1.0', 'negative':'-1.0', 'neutral': '0.0', 'irrelevant': '0.0'}, regex=True)\n",
    "tweets['label'] = pd.to_numeric(tweets['label'], errors='coerce')\n",
    "\n",
    "videos = pd.concat([okgo, trump, swift, royal, paul], ignore_index=True)\n",
    "data = videos.copy()\n",
    "data = fix_cols(data)\n",
    "\n",
    "\n",
    "df.columns = [\"comment\", \"label\"]\n",
    "\n",
    "#DataList = [videos, full, videos_not_royal, videos_not_okgo]\n",
    "#excluded = [okgo, royal]\n",
    "\n",
    "# clean up textual data (remove symbols)\n",
    "def AsStr(DF):\n",
    "    DF[\"comment\"]= DF[\"comment\"].astype(str)\n",
    "\n",
    "AsStr(data)\n",
    "AsStr(df)\n",
    "\n",
    "def cleanerFn(b):\n",
    "    for row in range(len(b)):\n",
    "        line = b.loc[row, \"comment\"]\n",
    "        b.loc[row,\"comment\"] = re.sub(\"[^a-zA-Z]\", \" \", line)\n",
    "\n",
    "cleanerFn(df)\n",
    "cleanerFn(data)\n",
    "data = data.dropna()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>Everyone knows brand s papers from  But  No on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Your paper cut balance is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>OH SHIT WHEN I SAW THIS ON MY FRONT PAGE      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Blowing my mind yet again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Should have gone with Dunder Mifflin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Made me smile  Great work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>The mad methodical geniuses do it again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Am I the only person who actually loves their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>The ad was the best   the store owner saved al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0   -1.0  Everyone knows brand s papers from  But  No on...\n",
       "1    0.0         Your paper cut balance is                 \n",
       "2    1.0  OH SHIT WHEN I SAW THIS ON MY FRONT PAGE      ...\n",
       "3    1.0                          Blowing my mind yet again\n",
       "4    0.0               Should have gone with Dunder Mifflin\n",
       "5    1.0                          Made me smile  Great work\n",
       "6    1.0                                                nan\n",
       "7    1.0           The mad methodical geniuses do it again \n",
       "8    1.0  Am I the only person who actually loves their ...\n",
       "9    1.0  The ad was the best   the store owner saved al..."
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andiedonovan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/andiedonovan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andiedonovan/myProjects/venv/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer\n",
    "import sklearn # machine learning\n",
    "from sklearn.model_selection import train_test_split # splitting up data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "def nlpFunction(a):\n",
    "    a['com_token']=a['comment'].str.lower().str.split()\n",
    "    a['com_remv']=a['com_token'].apply(lambda x: [y for y in x if y not in sw])\n",
    "    a[\"com_lemma\"] = a['com_remv'].apply(lambda x : [lemmatizer.lemmatize(y) for y in x]) # lemmatization\n",
    "    a['com_stem']=a['com_lemma'].apply(lambda x : [ps.stem(y) for y in x]) # stemming\n",
    "    a[\"com_stem_str\"] = a[\"com_stem\"].apply(', '.join)\n",
    "    return a\n",
    "\n",
    "df = nlpFunction(df)\n",
    "data = nlpFunction(data)\n",
    "trump = nlpFunction(trump)\n",
    "\n",
    "X_train = data[\"com_stem_str\"]\n",
    "X_test = trump[\"com_stem_str\"]\n",
    "Y_train = data[\"label\"]\n",
    "Y_test = trump[\"label\"]\n",
    "X_user = df[\"com_stem_str\"]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "xtrain = tfidf.fit_transform(X_train) # transform and fit training data\n",
    "xtest = tfidf.transform(X_test) # transform test data from fitted transformer\n",
    "xuser = tfidf.transform(X_user)\n",
    "data_trans= tfidf.transform(data[\"com_stem_str\"]) # transform entire dataset for cross validation\n",
    "df_trans = tfidf.transform(df[\"com_stem_str\"])\n",
    "\n",
    "'''X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                                    df[\"com_stem_str\"], df[\"label\"],\n",
    "                                    test_size=0.25,\n",
    "                                    random_state=42)'''\n",
    "\n",
    "\n",
    "# running models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm # support vector machine\n",
    "from sklearn import metrics # for accuracy/ precision\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier # Stochastic Gradient Descent\n",
    "from sklearn.neighbors import KNeighborsClassifier # k-NN ensemble method\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rs = 10\n",
    "lr = LogisticRegression(solver='sag', max_iter=100, random_state=rs, multi_class=\"multinomial\")\n",
    "mnb = MultinomialNB()\n",
    "svm = svm.SVC()\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=rs)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "models = ['lr', 'mnb', 'svm', 'rf', 'knn']\n",
    "labels = ['label_' + str(models[i]) for i in range(0,len(models))]\n",
    "predictions = [str(models[i])+\"_predict\" for i in range(0,len(models))]\n",
    "d = {}\n",
    "initModels = [lr, mnb, svm, rf, knn]\n",
    "\n",
    "for i in range(0,5):\n",
    "    initModels[i].fit(xtrain, Y_train)\n",
    "    d[predictions[i]] = initModels[i].predict(xuser)\n",
    "\n",
    "    # Create table of prediction accuracy rates\n",
    "Table = pd.DataFrame(columns=['comment', 'label_lr', 'label_mnb', 'label_svm', 'label_rf', 'label_knn'])\n",
    "for i in range(0, len(models)):\n",
    "    Table[labels[i]] = d[predictions[i]]\n",
    "Table[\"comment\"] = df[\"comment\"]\n",
    "\n",
    "# Create table of predicted sentiment ratios\n",
    "Ratios = pd.DataFrame(columns=['label_lr', 'label_mnb', 'label_svm', 'label_rf', 'label_knn'],\n",
    "    index=range(0,3))\n",
    "def RatioFinder(model):\n",
    "    pos = Table[Table[model]== 1.0]\n",
    "    neg = Table[Table[model]== -1.0]\n",
    "    neu = Table[Table[model]== 0.0]\n",
    "\n",
    "    pos_len = len(pos); neg_len = len(neg); neu_len = len(neu)\n",
    "    total = pos_len + neg_len + neu_len\n",
    "\n",
    "    neg_ratio = round(neg_len / float(total), 2) * 100\n",
    "    pos_ratio = round(pos_len / float(total), 2) * 100\n",
    "    neu_ratio = round(neu_len / float(total), 2) * 100\n",
    "\n",
    "    ratios = [pos_ratio, neu_ratio, neg_ratio]\n",
    "    return ratios\n",
    "\n",
    "for i in range(0,3):\n",
    "        for j in range(0,5):\n",
    "            Ratios.iloc[i,j] = RatioFinder(labels[j])[i]\n",
    "\n",
    "all_models = pd.DataFrame(columns=['average'], index=range(0,3))\n",
    "all_models[\"average\"]= Ratios.mean(axis=1)\n",
    "\n",
    "# set the prediction to the mode of the row\n",
    "Table[\"Prediction\"] = 0\n",
    "Table[\"Prediction\"] = Table[['label_lr','label_mnb','label_svm','label_rf','label_knn']].mode(axis=1)\n",
    "df.label = Table[\"Prediction\"]\n",
    "\n",
    "# extracting comments for each label\n",
    "df[\"com_remv\"] = df[\"com_remv\"].apply(', '.join)\n",
    "df[\"com_remv\"] = df[\"com_remv\"].str.replace(\",\",\"\").astype(str)\n",
    "\n",
    "'''df_words = df[[\"label\",\"com_remv\"]]\n",
    "positive = df_words[df_words[\"label\"]==1.0]\n",
    "neutral = df_words[df_words[\"label\"]==0.0]\n",
    "negative = df_words[df_words[\"label\"]==-1.0]\n",
    "'''\n",
    "p = df[df[\"label\"]==1]\n",
    "positive = p[\"com_remv\"]\n",
    "n = df[df[\"label\"]==-1]\n",
    "negative = n[\"com_remv\"]\n",
    "ne = df[df[\"label\"]==0]\n",
    "neutral = ne[\"com_remv\"]\n",
    "\n",
    "# most frequent words in each label\n",
    "most_freq_pos = pd.Series(' '.join(positive).lower().split()).value_counts()[:10]\n",
    "most_freq_neg = pd.Series(' '.join(negative).lower().split()).value_counts()[:10]\n",
    "most_freq_neu = pd.Series(' '.join(neutral).lower().split()).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=10, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16364692,  0.16364692, -0.08362049, ..., -0.00442806,\n",
       "        -0.00442806, -0.04655114],\n",
       "       [-0.10797118, -0.10797118,  0.25523527, ...,  0.00820924,\n",
       "         0.00820924,  0.08018729],\n",
       "       [-0.05567573, -0.05567573, -0.17161478, ..., -0.00378118,\n",
       "        -0.00378118, -0.03363616]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.37580726,  0.66650781, -0.29070055])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
